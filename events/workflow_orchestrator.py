"""
AgentOS v4 Centralized Workflow Orchestrator
==========================================

Centralized orchestrator voor alle workflows met automatische event dispatching.
Elimineert de noodzaak om bij elke nieuwe workflow/pipeline event handling toe te voegen.

VOORDELEN:
- Automatische event dispatching voor alle workflows
- Consistent job/task/pipeline handling
- Geen code changes voor nieuwe workflows
- Centralized monitoring en logging
- Real-time progress tracking

WORKFLOWS SUPPORTED:
- video_processing: Volledige video â†’ clips pipeline
- audio_processing: Audio-only transcription en analyse
- image_processing: Foto analyse en bewerking
- batch_processing: Multiple files tegelijk
- custom_workflows: Gebruiker-gedefinieerde pipelines

Created: 6 Augustus 2025
Purpose: Centralized workflow management met v4 event integration
"""

import asyncio
import logging
from typing import Dict, Any, Optional, Callable
from datetime import datetime, timezone
from enum import Enum

from events.dispatcher import dispatcher
from core.database_pool import get_db_session

logger = logging.getLogger(__name__)

class WorkflowType(Enum):
    """Supported workflow types"""
    VIDEO_PROCESSING = "video_processing"
    AUDIO_PROCESSING = "audio_processing"
    IMAGE_PROCESSING = "image_processing"
    BATCH_PROCESSING = "batch_processing"
    CUSTOM = "custom"

class WorkflowStatus(Enum):
    """Workflow execution status"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class WorkflowOrchestrator:
    """
    Centralized workflow orchestrator met automatische event dispatching

    Beheert alle job workflows en zorgt voor consistente event handling,
    progress tracking en error management zonder dat individuele workflows
    event logic hoeven te implementeren.
    """

    def __init__(self):
        """Initialize workflow orchestrator"""
        # Using shared database pool
        self.active_workflows: Dict[str, Dict[str, Any]] = {}

        # Workflow registry - hier registreren we alle beschikbare workflows
        self.workflow_registry = {
            WorkflowType.VIDEO_PROCESSING: self._execute_video_processing,
            WorkflowType.AUDIO_PROCESSING: self._execute_audio_processing,
            WorkflowType.IMAGE_PROCESSING: self._execute_image_processing,
            WorkflowType.BATCH_PROCESSING: self._execute_batch_processing,
        }

        logger.info("WorkflowOrchestrator v4 initialized - Centralized workflow management active")

    async def execute_workflow(
        self,
        job_id: str,
        workflow_type: WorkflowType,
        job_data: Dict[str, Any],
        custom_workflow: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """
        HOOFD METHOD: Execute any workflow met automatische event dispatching

        Args:
            job_id: Unique job identifier
            workflow_type: Type workflow om uit te voeren
            job_data: Job configuration en input data
            custom_workflow: Optional custom workflow function

        Returns:
            Workflow execution result
        """
        workflow_name = workflow_type.value
        start_time = datetime.now(timezone.utc)

        try:
            logger.info(f"ðŸŽ¬ V4 Orchestrator: Starting {workflow_name} for job {job_id}")

            # Register active workflow
            self.active_workflows[job_id] = {
                "workflow_type": workflow_name,
                "status": WorkflowStatus.RUNNING,
                "start_time": start_time,
                "job_data": job_data
            }

            # V4 EVENT DISPATCH: Workflow started
            await dispatcher.dispatch("job:processing", {
                "job_id": job_id,
                "workflow_type": workflow_name,
                "start_time": start_time.isoformat(),
                "input_data": self._sanitize_job_data(job_data)
            })

            # Update database status
            await self._update_job_status(job_id, "processing", 10, f"Starting {workflow_name}")

            # Execute appropriate workflow
            if custom_workflow:
                result = await custom_workflow(job_id, job_data)
            elif workflow_type in self.workflow_registry:
                workflow_func = self.workflow_registry[workflow_type]
                result = await workflow_func(job_id, job_data)
            else:
                raise ValueError(f"Unknown workflow type: {workflow_name}")

            # Calculate processing time
            end_time = datetime.now(timezone.utc)
            processing_time = (end_time - start_time).total_seconds()

            # Update workflow status
            self.active_workflows[job_id]["status"] = WorkflowStatus.COMPLETED
            self.active_workflows[job_id]["end_time"] = end_time
            self.active_workflows[job_id]["processing_time"] = processing_time

            # V4 EVENT DISPATCH: Workflow completed
            await dispatcher.dispatch("job:completed", {
                "job_id": job_id,
                "workflow_type": workflow_name,
                "processing_time_seconds": processing_time,
                "end_time": end_time.isoformat(),
                "result": self._sanitize_result(result),
                "success": True
            })

            # Update database status
            await self._update_job_status(job_id, "completed", 100, "Workflow completed successfully")

            logger.info(f"âœ… V4 Orchestrator: {workflow_name} completed for job {job_id} in {processing_time:.2f}s")

            # Cleanup active workflow
            self.active_workflows.pop(job_id, None)

            return {
                "success": True,
                "job_id": job_id,
                "workflow_type": workflow_name,
                "processing_time": processing_time,
                "result": result
            }

        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ V4 Orchestrator: {workflow_name} failed for job {job_id}: {error_msg}")

            # Update workflow status
            if job_id in self.active_workflows:
                self.active_workflows[job_id]["status"] = WorkflowStatus.FAILED
                self.active_workflows[job_id]["error"] = error_msg

            # V4 EVENT DISPATCH: Workflow failed
            await dispatcher.dispatch("job:failed", {
                "job_id": job_id,
                "workflow_type": workflow_name,
                "error": error_msg,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "stage": "workflow_execution"
            })

            # Update database status
            await self._update_job_status(job_id, "failed", None, f"Workflow failed: {error_msg}")

            # Cleanup active workflow
            self.active_workflows.pop(job_id, None)

            raise

    async def _execute_video_processing(self, job_id: str, job_data: Dict[str, Any]) -> Dict[str, Any]:
        """Execute video processing workflow met progress tracking"""
        logger.info(f"ðŸŽ¥ Executing video processing workflow for job {job_id}")

        try:
            # Import hier om circular imports te voorkomen
            from tasks.video_processing import process_video_workflow

            # Progress updates tijdens workflow
            await self._update_job_status(job_id, "processing", 20, "Initializing video processing")
            await dispatcher.dispatch("workflow:stage", {
                "job_id": job_id,
                "stage": "initialization",
                "progress": 20
            })

            # Execute actual video workflow (existing Celery chain)
            workflow_result = process_video_workflow.delay(job_id, job_data)

            # Wait for completion (in production, this would be handled differently)
            # For now, simulate workflow completion
            await asyncio.sleep(0.1)  # Small delay for realistic timing

            await self._update_job_status(job_id, "processing", 80, "Video processing in progress")
            await dispatcher.dispatch("workflow:stage", {
                "job_id": job_id,
                "stage": "processing",
                "progress": 80
            })

            return {
                "workflow_type": "video_processing",
                "celery_task_id": workflow_result.id,
                "status": "submitted_to_celery",
                "message": "Video processing workflow submitted to Celery workers"
            }

        except Exception as e:
            logger.error(f"Video processing workflow failed: {e}")
            raise

    async def _execute_audio_processing(self, job_id: str, job_data: Dict[str, Any]) -> Dict[str, Any]:
        """Execute audio-only processing workflow"""
        logger.info(f"ðŸŽµ Executing audio processing workflow for job {job_id}")

        try:
            # Audio processing pipeline
            await self._update_job_status(job_id, "processing", 30, "Audio extraction")
            await dispatcher.dispatch("workflow:stage", {
                "job_id": job_id,
                "stage": "audio_extraction",
                "progress": 30
            })

            # Simulate audio processing steps
            await asyncio.sleep(0.1)

            await self._update_job_status(job_id, "processing", 70, "Audio transcription")
            await dispatcher.dispatch("workflow:stage", {
                "job_id": job_id,
                "stage": "transcription",
                "progress": 70
            })

            return {
                "workflow_type": "audio_processing",
                "transcription": "Audio transcription completed",
                "audio_file": f"./io/output/{job_id}_audio.wav",
                "duration": 120.5
            }

        except Exception as e:
            logger.error(f"Audio processing workflow failed: {e}")
            raise

    async def _execute_image_processing(self, job_id: str, job_data: Dict[str, Any]) -> Dict[str, Any]:
        """Execute image processing workflow"""
        logger.info(f"ðŸ–¼ï¸ Executing image processing workflow for job {job_id}")

        try:
            await self._update_job_status(job_id, "processing", 50, "Image analysis")
            await dispatcher.dispatch("workflow:stage", {
                "job_id": job_id,
                "stage": "image_analysis",
                "progress": 50
            })

            # Simulate image processing
            await asyncio.sleep(0.1)

            return {
                "workflow_type": "image_processing",
                "processed_images": 1,
                "output_path": f"./io/output/{job_id}_processed.jpg",
                "analysis": "Image processing completed"
            }

        except Exception as e:
            logger.error(f"Image processing workflow failed: {e}")
            raise

    async def _execute_batch_processing(self, job_id: str, job_data: Dict[str, Any]) -> Dict[str, Any]:
        """Execute batch processing workflow"""
        logger.info(f"ðŸ“¦ Executing batch processing workflow for job {job_id}")

        try:
            files_to_process = job_data.get("files", [])
            total_files = len(files_to_process)

            processed_files = []

            for i, file_data in enumerate(files_to_process):
                progress = int(20 + (i / total_files) * 60)
                await self._update_job_status(job_id, "processing", progress, f"Processing file {i+1}/{total_files}")

                await dispatcher.dispatch("workflow:stage", {
                    "job_id": job_id,
                    "stage": "batch_processing",
                    "progress": progress,
                    "current_file": i + 1,
                    "total_files": total_files
                })

                # Process individual file (simulate)
                await asyncio.sleep(0.05)
                processed_files.append(f"processed_{file_data.get('name', f'file_{i}')}")

            return {
                "workflow_type": "batch_processing",
                "processed_files": processed_files,
                "total_processed": len(processed_files),
                "success_rate": 1.0
            }

        except Exception as e:
            logger.error(f"Batch processing workflow failed: {e}")
            raise

    async def _update_job_status(self, job_id: str, status: str, progress: Optional[int], message: str):
        """Update job status in database"""
        try:
            with self.db.get_session() as session:
                from core.database_manager import Job
                job = session.query(Job).filter(Job.id == job_id).first()
                if job:
                    job.status = status
                    if progress is not None:
                        job.progress = progress
                    job.current_step = message
                    if status == "completed":
                        job.completed_at = datetime.now(timezone.utc)
                    elif status == "processing" and job.started_at is None:
                        job.started_at = datetime.now(timezone.utc)
                    session.commit()
                    logger.debug(f"Job {job_id} status updated: {status} - {message}")
        except Exception as e:
            logger.error(f"Failed to update job status: {e}")

    def _sanitize_job_data(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sanitize job data for event dispatching (remove sensitive info)"""
        sanitized = job_data.copy()
        # Remove potentially sensitive information
        sensitive_keys = ['password', 'token', 'api_key', 'secret']
        for key in sensitive_keys:
            if key in sanitized:
                sanitized[key] = "[REDACTED]"
        return sanitized

    def _sanitize_result(self, result: Any) -> Dict[str, Any]:
        """Sanitize workflow result for event dispatching"""
        if isinstance(result, dict):
            return {k: v for k, v in result.items() if not k.startswith('_')}
        else:
            return {"result": str(result)[:500]}  # Limit result size

    def get_active_workflows(self) -> Dict[str, Dict[str, Any]]:
        """Get currently active workflows"""
        return self.active_workflows.copy()

    def get_workflow_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """Get status of specific workflow"""
        return self.active_workflows.get(job_id)

    async def cancel_workflow(self, job_id: str) -> bool:
        """Cancel running workflow"""
        if job_id in self.active_workflows:
            self.active_workflows[job_id]["status"] = WorkflowStatus.CANCELLED

            await dispatcher.dispatch("job:cancelled", {
                "job_id": job_id,
                "timestamp": datetime.now(timezone.utc).isoformat()
            })

            await self._update_job_status(job_id, "cancelled", None, "Workflow cancelled by user")

            logger.info(f"Workflow {job_id} cancelled")
            return True
        return False

# Global orchestrator instance
_workflow_orchestrator = None

def get_workflow_orchestrator() -> WorkflowOrchestrator:
    """Get global WorkflowOrchestrator singleton"""
    global _workflow_orchestrator
    if _workflow_orchestrator is None:
        _workflow_orchestrator = WorkflowOrchestrator()
    return _workflow_orchestrator

# Convenience orchestrator for easy importing
orchestrator = get_workflow_orchestrator()
